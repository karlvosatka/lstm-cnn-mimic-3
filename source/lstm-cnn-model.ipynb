{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# LSTM+CNN Model\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from dataloader import dataset, load_data\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import WeightedRandomSampler, SubsetRandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## LSTM+CNN Model Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class LSTMPlusCNN(nn.Module):\n",
    "    def __init__(self, input_size, input_len, feature_len, hidden_size, dropout=0.5):\n",
    "        super(LSTMPlusCNN, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size,\n",
    "                            hidden_size,\n",
    "                            num_layers=3,\n",
    "                            batch_first=True,\n",
    "                            dropout=dropout, # dropout percentage not specified in paper\n",
    "                            bidirectional=True)\n",
    "        self.bn = nn.BatchNorm1d(2*hidden_size)\n",
    "        self.cnn = nn.Conv1d(in_channels=2*hidden_size,\n",
    "                             out_channels=3, # paper refers to \"No of filters: n\"\n",
    "                             kernel_size=3)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        linear_features = feature_len + 3*(input_len - 2)//2\n",
    "        self.fc = nn.Linear(in_features=linear_features,\n",
    "                            out_features=2)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    def forward(self, x, feats, masks):\n",
    "        batch_size, seq_len = x.shape[:2]\n",
    "        x = torch.reshape(x, (batch_size, seq_len, -1))\n",
    "        masks = torch.reshape(masks, (batch_size, seq_len, -1))\n",
    "        x, _ = self.lstm(x)\n",
    "        x = torch.movedim(x, 1, 2) # (N,L,C) -> (N,C,L)\n",
    "        x = self.bn(x)\n",
    "        x = self.cnn(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x_plus_feats = torch.cat((x, feats), dim=1)\n",
    "        x = self.fc(x_plus_feats)\n",
    "        out = self.softmax(x) # (N,C*L) -> (N,2)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.4995, 0.5005],\n        [0.4862, 0.5138],\n        [0.4920, 0.5080],\n        [0.4900, 0.5100],\n        [0.4822, 0.5178],\n        [0.5069, 0.4931],\n        [0.4880, 0.5120],\n        [0.4780, 0.5220],\n        [0.4579, 0.5421],\n        [0.4929, 0.5071]], grad_fn=<SoftmaxBackward>)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# idx = np.arange(500)\n",
    "# sampler = subset_weighted_random_sampler(dataset, idx)\n",
    "# train_dl, val_dl = load_data(sampler, sampler, batch_size=64)\n",
    "# x, feats, masks, y = next(iter(train_dl))\n",
    "# our_input_size = 3926\n",
    "# model = LSTMPlusCNN(input_size=3926, input_len=48, feature_len=4037, hidden_size=128)\n",
    "# model(x, feats, masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## LSTM+CNN Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Define Training and Validation Functions\n",
    "adapted from: https://medium.com/dataseries/k-fold-cross-validation-with-pytorch-and-sklearn-d094aa00105f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, train_dataloader, optimizer, criterion):\n",
    "    train_loss, train_correct = 0.0, 0\n",
    "    train_false_pos, train_false_neg, train_true_pos, train_true_neg = 0.0, 0.0, 0.0, 0.0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for x, feats, masks, y in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        y_hat = model(x, feats, masks)\n",
    "        y_pred = torch.argmax(y_hat, dim=1)\n",
    "        loss = criterion(y_hat, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_correct += (y_pred == y).sum().item()\n",
    "        train_false_pos += ((y_pred == 1) * (y == 0)).sum()\n",
    "        train_true_pos += ((y_pred == 1) * (y == 1)).sum()\n",
    "        train_false_neg += ((y_pred == 0) * (y == 1)).sum()\n",
    "        train_true_neg += ((y_pred == 0) * (y == 0)).sum()\n",
    "\n",
    "    return train_loss, train_correct, train_false_pos, train_false_neg, train_true_pos, train_true_neg\n",
    "\n",
    "def valid_epoch(model, valid_dataloader, criterion):\n",
    "    valid_loss, valid_correct = 0.0, 0\n",
    "    valid_false_pos, valid_false_neg, valid_true_pos, valid_true_neg = 0.0, 0.0, 0.0, 0.0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, feats, masks, y in valid_dataloader:\n",
    "            y_hat = model(x, feats, masks)\n",
    "            y_pred = torch.argmax(y_hat, dim=1)\n",
    "            loss = criterion(y_hat, y)\n",
    "\n",
    "            valid_loss += loss.item()\n",
    "            valid_correct += (y_pred == y).sum().item()\n",
    "            valid_false_pos += ((y_pred == 1) * (y == 0)).sum()\n",
    "            valid_true_pos += ((y_pred == 1) * (y == 1)).sum()\n",
    "            valid_false_neg += ((y_pred == 0) * (y == 1)).sum()\n",
    "            valid_true_neg += ((y_pred == 0) * (y == 0)).sum()\n",
    "\n",
    "    return valid_loss, valid_correct, valid_false_pos, valid_false_neg, valid_true_pos, valid_true_neg\n",
    "\n",
    "def subset_weighted_random_sampler(dataset, idx, sample=\"over\"):\n",
    "    labels = torch.tensor(dataset.y)\n",
    "    subset_labels = labels[idx]\n",
    "    majority_len = int((subset_labels == 0).sum())\n",
    "    minority_len = int((subset_labels == 1).sum())\n",
    "    if sample == \"over\":\n",
    "        sample_size = 2 * majority_len\n",
    "    elif sample == \"under\":\n",
    "        sample_size = majority_len\n",
    "    else:\n",
    "        sample_size = len(idx)\n",
    "\n",
    "    dist = torch.zeros(len(dataset.y))\n",
    "    dist[idx] = 1\n",
    "    dist[labels == 1] = dist[labels == 1] * (0.5 / minority_len)\n",
    "    dist[labels == 0] = dist[labels == 0] * (0.5 / majority_len)\n",
    "\n",
    "    return WeightedRandomSampler(dist, num_samples=sample_size, replacement=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## K Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def kfold(dataset=dataset, k_folds=5, n_epochs=10, batch_size=64):\n",
    "    history = {'train_loss': [], 'test_loss': [],'train_acc':[],'test_acc':[], 'train_tpr':[], 'test_tpr':[], 'train_fpr':[], 'test_fpr':[]}\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True)\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(np.arange(len(dataset)))):\n",
    "        print('Fold {}'.format(fold + 1))\n",
    "\n",
    "        train_sampler = subset_weighted_random_sampler(dataset, train_idx)\n",
    "        test_sampler = subset_weighted_random_sampler(dataset, val_idx)\n",
    "        train_loader, test_loader = load_data(train_sampler, test_sampler, batch_size=batch_size)\n",
    "\n",
    "        model = LSTMPlusCNN(input_size=3926, input_len=48, feature_len=4037, hidden_size=128)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.002)\n",
    "        #criterion = FocalLoss(gamma=0.2, alpha=0.75)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            train_loss, train_correct, train_fpos, train_fneg, train_tpos, train_tneg = train_epoch(model, train_loader, optimizer, criterion)\n",
    "            test_loss, test_correct, test_fpos, test_fneg, test_tpos, test_tneg = valid_epoch(model, test_loader, criterion)\n",
    "\n",
    "            train_loss = train_loss / len(train_loader.sampler)\n",
    "            train_acc = train_correct / len(train_loader.sampler) * 100\n",
    "            test_loss = test_loss / len(test_loader.sampler)\n",
    "            test_acc = test_correct / len(test_loader.sampler) * 100\n",
    "            train_tpr = train_tpos / (train_tpos + train_fneg)\n",
    "            test_tpr = test_tpos / (test_tpos + test_fneg)\n",
    "            train_fpr = train_fpos / (train_fpos + train_tneg)\n",
    "            test_fpr = test_fpos / (test_fpos + test_tneg)\n",
    "\n",
    "            print(\"Epoch:{}/{} AVG Training Loss:{:.3f} AVG Test Loss:{:.3f} AVG Training Acc {:.2f} % AVG Test Acc {:.2f} %\".format(epoch + 1, n_epochs, train_loss, test_loss, train_acc, test_acc))\n",
    "            print(\"Epoch:{}/{} AVG Training TPR:{:.3f} AVG Test TPR:{:.3f} AVG Training FPR:{:.3f} AVG Test FPR:{:.3f}\".format(epoch + 1, n_epochs, train_tpr, test_tpr, train_fpr, train_fpr))\n",
    "            history['train_loss'].append(train_loss)\n",
    "            history['test_loss'].append(test_loss)\n",
    "            history['train_acc'].append(train_acc)\n",
    "            history['test_acc'].append(test_acc)\n",
    "            history['train_tpr'].append(train_tpr)\n",
    "            history['test_tpr'].append(test_tpr)\n",
    "            history['train_fpr'].append(train_fpr)\n",
    "            history['test_fpr'].append(test_fpr)\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Epoch:1/10 AVG Training Loss:0.010 AVG Test Loss:0.010 AVG Training Acc 68.41 % AVG Test Acc 69.04 %\n",
      "Epoch:1/10 AVG Training TPR:0.664 AVG Test TPR:0.785 AVG Training FPR:0.295 AVG Test FPR:0.295\n",
      "Epoch:2/10 AVG Training Loss:0.009 AVG Test Loss:0.010 AVG Training Acc 76.25 % AVG Test Acc 67.24 %\n",
      "Epoch:2/10 AVG Training TPR:0.780 AVG Test TPR:0.624 AVG Training FPR:0.255 AVG Test FPR:0.255\n",
      "Epoch:3/10 AVG Training Loss:0.008 AVG Test Loss:0.009 AVG Training Acc 82.10 % AVG Test Acc 70.08 %\n",
      "Epoch:3/10 AVG Training TPR:0.841 AVG Test TPR:0.658 AVG Training FPR:0.199 AVG Test FPR:0.199\n",
      "Epoch:4/10 AVG Training Loss:0.008 AVG Test Loss:0.009 AVG Training Acc 83.50 % AVG Test Acc 74.12 %\n",
      "Epoch:4/10 AVG Training TPR:0.857 AVG Test TPR:0.714 AVG Training FPR:0.188 AVG Test FPR:0.188\n",
      "Epoch:5/10 AVG Training Loss:0.007 AVG Test Loss:0.009 AVG Training Acc 86.87 % AVG Test Acc 68.66 %\n",
      "Epoch:5/10 AVG Training TPR:0.899 AVG Test TPR:0.626 AVG Training FPR:0.165 AVG Test FPR:0.165\n",
      "Epoch:6/10 AVG Training Loss:0.007 AVG Test Loss:0.009 AVG Training Acc 87.45 % AVG Test Acc 69.41 %\n",
      "Epoch:6/10 AVG Training TPR:0.892 AVG Test TPR:0.662 AVG Training FPR:0.143 AVG Test FPR:0.143\n",
      "Epoch:7/10 AVG Training Loss:0.007 AVG Test Loss:0.010 AVG Training Acc 89.23 % AVG Test Acc 66.64 %\n",
      "Epoch:7/10 AVG Training TPR:0.922 AVG Test TPR:0.526 AVG Training FPR:0.137 AVG Test FPR:0.137\n",
      "Epoch:8/10 AVG Training Loss:0.007 AVG Test Loss:0.009 AVG Training Acc 89.87 % AVG Test Acc 71.28 %\n",
      "Epoch:8/10 AVG Training TPR:0.932 AVG Test TPR:0.631 AVG Training FPR:0.136 AVG Test FPR:0.136\n",
      "Epoch:9/10 AVG Training Loss:0.007 AVG Test Loss:0.010 AVG Training Acc 91.22 % AVG Test Acc 64.85 %\n",
      "Epoch:9/10 AVG Training TPR:0.934 AVG Test TPR:0.453 AVG Training FPR:0.111 AVG Test FPR:0.111\n",
      "Epoch:10/10 AVG Training Loss:0.007 AVG Test Loss:0.010 AVG Training Acc 92.21 % AVG Test Acc 68.59 %\n",
      "Epoch:10/10 AVG Training TPR:0.948 AVG Test TPR:0.535 AVG Training FPR:0.104 AVG Test FPR:0.104\n",
      "Fold 2\n",
      "Epoch:1/10 AVG Training Loss:0.010 AVG Test Loss:0.009 AVG Training Acc 67.79 % AVG Test Acc 72.70 %\n",
      "Epoch:1/10 AVG Training TPR:0.707 AVG Test TPR:0.694 AVG Training FPR:0.353 AVG Test FPR:0.353\n",
      "Epoch:2/10 AVG Training Loss:0.009 AVG Test Loss:0.009 AVG Training Acc 77.04 % AVG Test Acc 75.29 %\n",
      "Epoch:2/10 AVG Training TPR:0.782 AVG Test TPR:0.800 AVG Training FPR:0.241 AVG Test FPR:0.241\n",
      "Epoch:3/10 AVG Training Loss:0.008 AVG Test Loss:0.009 AVG Training Acc 81.50 % AVG Test Acc 75.51 %\n",
      "Epoch:3/10 AVG Training TPR:0.850 AVG Test TPR:0.732 AVG Training FPR:0.222 AVG Test FPR:0.222\n",
      "Epoch:4/10 AVG Training Loss:0.008 AVG Test Loss:0.009 AVG Training Acc 83.77 % AVG Test Acc 70.27 %\n",
      "Epoch:4/10 AVG Training TPR:0.857 AVG Test TPR:0.519 AVG Training FPR:0.181 AVG Test FPR:0.181\n",
      "Epoch:5/10 AVG Training Loss:0.007 AVG Test Loss:0.009 AVG Training Acc 86.39 % AVG Test Acc 74.98 %\n",
      "Epoch:5/10 AVG Training TPR:0.881 AVG Test TPR:0.715 AVG Training FPR:0.154 AVG Test FPR:0.154\n",
      "Epoch:6/10 AVG Training Loss:0.007 AVG Test Loss:0.009 AVG Training Acc 88.33 % AVG Test Acc 73.38 %\n",
      "Epoch:6/10 AVG Training TPR:0.899 AVG Test TPR:0.619 AVG Training FPR:0.132 AVG Test FPR:0.132\n",
      "Epoch:7/10 AVG Training Loss:0.007 AVG Test Loss:0.009 AVG Training Acc 89.39 % AVG Test Acc 73.00 %\n",
      "Epoch:7/10 AVG Training TPR:0.923 AVG Test TPR:0.596 AVG Training FPR:0.136 AVG Test FPR:0.136\n",
      "Epoch:8/10 AVG Training Loss:0.007 AVG Test Loss:0.009 AVG Training Acc 90.62 % AVG Test Acc 72.32 %\n",
      "Epoch:8/10 AVG Training TPR:0.933 AVG Test TPR:0.603 AVG Training FPR:0.120 AVG Test FPR:0.120\n",
      "Epoch:9/10 AVG Training Loss:0.007 AVG Test Loss:0.009 AVG Training Acc 91.81 % AVG Test Acc 71.56 %\n",
      "Epoch:9/10 AVG Training TPR:0.939 AVG Test TPR:0.557 AVG Training FPR:0.103 AVG Test FPR:0.103\n",
      "Epoch:10/10 AVG Training Loss:0.006 AVG Test Loss:0.010 AVG Training Acc 93.10 % AVG Test Acc 65.63 %\n",
      "Epoch:10/10 AVG Training TPR:0.953 AVG Test TPR:0.454 AVG Training FPR:0.091 AVG Test FPR:0.091\n",
      "Fold 3\n",
      "Epoch:1/10 AVG Training Loss:0.010 AVG Test Loss:0.010 AVG Training Acc 69.06 % AVG Test Acc 69.14 %\n",
      "Epoch:1/10 AVG Training TPR:0.738 AVG Test TPR:0.587 AVG Training FPR:0.358 AVG Test FPR:0.358\n",
      "Epoch:2/10 AVG Training Loss:0.009 AVG Test Loss:0.010 AVG Training Acc 79.12 % AVG Test Acc 65.39 %\n",
      "Epoch:2/10 AVG Training TPR:0.797 AVG Test TPR:0.516 AVG Training FPR:0.214 AVG Test FPR:0.214\n",
      "Epoch:3/10 AVG Training Loss:0.008 AVG Test Loss:0.010 AVG Training Acc 82.19 % AVG Test Acc 66.29 %\n",
      "Epoch:3/10 AVG Training TPR:0.847 AVG Test TPR:0.578 AVG Training FPR:0.204 AVG Test FPR:0.204\n",
      "Epoch:4/10 AVG Training Loss:0.008 AVG Test Loss:0.010 AVG Training Acc 83.70 % AVG Test Acc 66.97 %\n",
      "Epoch:4/10 AVG Training TPR:0.852 AVG Test TPR:0.550 AVG Training FPR:0.177 AVG Test FPR:0.177\n",
      "Epoch:5/10 AVG Training Loss:0.007 AVG Test Loss:0.010 AVG Training Acc 85.65 % AVG Test Acc 66.89 %\n",
      "Epoch:5/10 AVG Training TPR:0.884 AVG Test TPR:0.609 AVG Training FPR:0.171 AVG Test FPR:0.171\n",
      "Epoch:6/10 AVG Training Loss:0.007 AVG Test Loss:0.010 AVG Training Acc 87.63 % AVG Test Acc 65.17 %\n",
      "Epoch:6/10 AVG Training TPR:0.902 AVG Test TPR:0.381 AVG Training FPR:0.149 AVG Test FPR:0.149\n",
      "Epoch:7/10 AVG Training Loss:0.007 AVG Test Loss:0.010 AVG Training Acc 89.34 % AVG Test Acc 67.64 %\n",
      "Epoch:7/10 AVG Training TPR:0.923 AVG Test TPR:0.646 AVG Training FPR:0.136 AVG Test FPR:0.136\n",
      "Epoch:8/10 AVG Training Loss:0.007 AVG Test Loss:0.010 AVG Training Acc 90.55 % AVG Test Acc 67.87 %\n",
      "Epoch:8/10 AVG Training TPR:0.936 AVG Test TPR:0.645 AVG Training FPR:0.126 AVG Test FPR:0.126\n",
      "Epoch:9/10 AVG Training Loss:0.007 AVG Test Loss:0.010 AVG Training Acc 90.72 % AVG Test Acc 65.17 %\n",
      "Epoch:9/10 AVG Training TPR:0.940 AVG Test TPR:0.489 AVG Training FPR:0.127 AVG Test FPR:0.127\n",
      "Epoch:10/10 AVG Training Loss:0.007 AVG Test Loss:0.011 AVG Training Acc 92.12 % AVG Test Acc 61.41 %\n",
      "Epoch:10/10 AVG Training TPR:0.950 AVG Test TPR:0.395 AVG Training FPR:0.109 AVG Test FPR:0.109\n",
      "Fold 4\n",
      "Epoch:1/10 AVG Training Loss:0.010 AVG Test Loss:0.010 AVG Training Acc 68.62 % AVG Test Acc 66.20 %\n",
      "Epoch:1/10 AVG Training TPR:0.684 AVG Test TPR:0.816 AVG Training FPR:0.311 AVG Test FPR:0.311\n",
      "Epoch:2/10 AVG Training Loss:0.009 AVG Test Loss:0.010 AVG Training Acc 77.73 % AVG Test Acc 72.96 %\n",
      "Epoch:2/10 AVG Training TPR:0.810 AVG Test TPR:0.767 AVG Training FPR:0.256 AVG Test FPR:0.256\n",
      "Epoch:3/10 AVG Training Loss:0.008 AVG Test Loss:0.010 AVG Training Acc 80.30 % AVG Test Acc 69.84 %\n",
      "Epoch:3/10 AVG Training TPR:0.821 AVG Test TPR:0.510 AVG Training FPR:0.215 AVG Test FPR:0.215\n",
      "Epoch:4/10 AVG Training Loss:0.008 AVG Test Loss:0.010 AVG Training Acc 83.31 % AVG Test Acc 66.57 %\n",
      "Epoch:4/10 AVG Training TPR:0.867 AVG Test TPR:0.646 AVG Training FPR:0.203 AVG Test FPR:0.203\n",
      "Epoch:5/10 AVG Training Loss:0.007 AVG Test Loss:0.010 AVG Training Acc 85.93 % AVG Test Acc 66.72 %\n",
      "Epoch:5/10 AVG Training TPR:0.885 AVG Test TPR:0.497 AVG Training FPR:0.166 AVG Test FPR:0.166\n",
      "Epoch:6/10 AVG Training Loss:0.007 AVG Test Loss:0.011 AVG Training Acc 88.16 % AVG Test Acc 64.04 %\n",
      "Epoch:6/10 AVG Training TPR:0.906 AVG Test TPR:0.448 AVG Training FPR:0.141 AVG Test FPR:0.141\n",
      "Epoch:7/10 AVG Training Loss:0.007 AVG Test Loss:0.010 AVG Training Acc 89.14 % AVG Test Acc 65.38 %\n",
      "Epoch:7/10 AVG Training TPR:0.915 AVG Test TPR:0.513 AVG Training FPR:0.133 AVG Test FPR:0.133\n",
      "Epoch:8/10 AVG Training Loss:0.007 AVG Test Loss:0.010 AVG Training Acc 89.78 % AVG Test Acc 67.01 %\n",
      "Epoch:8/10 AVG Training TPR:0.917 AVG Test TPR:0.530 AVG Training FPR:0.121 AVG Test FPR:0.121\n",
      "Epoch:9/10 AVG Training Loss:0.007 AVG Test Loss:0.010 AVG Training Acc 91.73 % AVG Test Acc 70.58 %\n",
      "Epoch:9/10 AVG Training TPR:0.941 AVG Test TPR:0.607 AVG Training FPR:0.107 AVG Test FPR:0.107\n",
      "Epoch:10/10 AVG Training Loss:0.006 AVG Test Loss:0.011 AVG Training Acc 92.53 % AVG Test Acc 63.08 %\n",
      "Epoch:10/10 AVG Training TPR:0.949 AVG Test TPR:0.465 AVG Training FPR:0.100 AVG Test FPR:0.100\n",
      "Fold 5\n",
      "Epoch:1/10 AVG Training Loss:0.010 AVG Test Loss:0.010 AVG Training Acc 69.08 % AVG Test Acc 68.82 %\n",
      "Epoch:1/10 AVG Training TPR:0.702 AVG Test TPR:0.628 AVG Training FPR:0.320 AVG Test FPR:0.320\n",
      "Epoch:2/10 AVG Training Loss:0.009 AVG Test Loss:0.010 AVG Training Acc 77.67 % AVG Test Acc 70.60 %\n",
      "Epoch:2/10 AVG Training TPR:0.770 AVG Test TPR:0.685 AVG Training FPR:0.217 AVG Test FPR:0.217\n",
      "Epoch:3/10 AVG Training Loss:0.008 AVG Test Loss:0.010 AVG Training Acc 80.84 % AVG Test Acc 68.08 %\n",
      "Epoch:3/10 AVG Training TPR:0.829 AVG Test TPR:0.473 AVG Training FPR:0.212 AVG Test FPR:0.212\n",
      "Epoch:4/10 AVG Training Loss:0.008 AVG Test Loss:0.010 AVG Training Acc 83.53 % AVG Test Acc 68.75 %\n",
      "Epoch:4/10 AVG Training TPR:0.853 AVG Test TPR:0.530 AVG Training FPR:0.183 AVG Test FPR:0.183\n",
      "Epoch:5/10 AVG Training Loss:0.008 AVG Test Loss:0.010 AVG Training Acc 86.87 % AVG Test Acc 67.93 %\n",
      "Epoch:5/10 AVG Training TPR:0.896 AVG Test TPR:0.543 AVG Training FPR:0.159 AVG Test FPR:0.159\n",
      "Epoch:6/10 AVG Training Loss:0.007 AVG Test Loss:0.010 AVG Training Acc 87.37 % AVG Test Acc 66.07 %\n",
      "Epoch:6/10 AVG Training TPR:0.914 AVG Test TPR:0.466 AVG Training FPR:0.166 AVG Test FPR:0.166\n",
      "Epoch:7/10 AVG Training Loss:0.007 AVG Test Loss:0.010 AVG Training Acc 87.95 % AVG Test Acc 67.48 %\n",
      "Epoch:7/10 AVG Training TPR:0.904 AVG Test TPR:0.629 AVG Training FPR:0.144 AVG Test FPR:0.144\n",
      "Epoch:8/10 AVG Training Loss:0.007 AVG Test Loss:0.010 AVG Training Acc 89.96 % AVG Test Acc 72.31 %\n",
      "Epoch:8/10 AVG Training TPR:0.932 AVG Test TPR:0.689 AVG Training FPR:0.133 AVG Test FPR:0.133\n",
      "Epoch:9/10 AVG Training Loss:0.007 AVG Test Loss:0.009 AVG Training Acc 90.83 % AVG Test Acc 72.31 %\n",
      "Epoch:9/10 AVG Training TPR:0.935 AVG Test TPR:0.654 AVG Training FPR:0.118 AVG Test FPR:0.118\n",
      "Epoch:10/10 AVG Training Loss:0.007 AVG Test Loss:0.009 AVG Training Acc 91.56 % AVG Test Acc 70.30 %\n",
      "Epoch:10/10 AVG Training TPR:0.948 AVG Test TPR:0.679 AVG Training FPR:0.118 AVG Test FPR:0.118\n",
      "CPU times: user 2h 54min 39s, sys: 2min 1s, total: 2h 56min 41s\n",
      "Wall time: 2h 50min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = kfold()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Storing Results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"./lstm_cnn_model_history_undersample.pickle\", \"wb\") as f:\n",
    "    pickle.dump(history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}