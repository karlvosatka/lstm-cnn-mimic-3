{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Logistic Regression"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from dataloader import dataset, load_data\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import WeightedRandomSampler, SubsetRandomSampler, SequentialSampler"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Logistic Regression Model Design"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, feature_size):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.fc = nn.Linear(feature_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, feats):\n",
    "        out = self.sigmoid(self.fc(feats.float()))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([ True,  True,  True, False,  True,  True,  True,  True,  True,  True])"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_dl, val_dl = load_data(SequentialSampler(dataset), SequentialSampler(dataset), batch_size=10)\n",
    "# x, feats, masks, y = next(iter(train_dl))\n",
    "# model = LogisticRegression(feature_size=4037)\n",
    "# y_hat = model(feats)\n",
    "# y_pred = torch.round(y_hat.detach())\n",
    "# y_pred.squeeze() == y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Logistic Regression Training\n",
    "Define Training and Validation Functions adapted from: https://medium.com/dataseries/k-fold-cross-validation-with-pytorch-and-sklearn-d094aa00105f"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def train_epoch(model, train_dataloader, optimizer, criterion):\n",
    "    train_loss, train_correct = 0.0, 0\n",
    "    train_false_pos, train_false_neg, train_true_pos, train_true_neg = 0.0, 0.0, 0.0, 0.0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for x, feats, masks, y in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        y_hat = model(feats)\n",
    "        y_pred = torch.round(y_hat.detach()).squeeze()\n",
    "        loss = criterion(y_hat, y.unsqueeze(1).float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_correct += (y_pred == y).sum().item()\n",
    "        train_false_pos += ((y_pred == 1) * (y == 0)).sum()\n",
    "        train_true_pos += ((y_pred == 1) * (y == 1)).sum()\n",
    "        train_false_neg += ((y_pred == 0) * (y == 1)).sum()\n",
    "        train_true_neg += ((y_pred == 0) * (y == 0)).sum()\n",
    "\n",
    "    return train_loss, train_correct, train_false_pos, train_false_neg, train_true_pos, train_true_neg\n",
    "\n",
    "def valid_epoch(model, valid_dataloader, criterion):\n",
    "    valid_loss, valid_correct = 0.0, 0\n",
    "    valid_false_pos, valid_false_neg, valid_true_pos, valid_true_neg = 0.0, 0.0, 0.0, 0.0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, feats, masks, y in valid_dataloader:\n",
    "            y_hat = model(feats)\n",
    "            y_pred = torch.round(y_hat.detach()).squeeze()\n",
    "            loss = criterion(y_hat, y.unsqueeze(1).float())\n",
    "\n",
    "            valid_loss += loss.item()\n",
    "            valid_correct += (y_pred == y).sum().item()\n",
    "            valid_false_pos += ((y_pred == 1) * (y == 0)).sum()\n",
    "            valid_true_pos += ((y_pred == 1) * (y == 1)).sum()\n",
    "            valid_false_neg += ((y_pred == 0) * (y == 1)).sum()\n",
    "            valid_true_neg += ((y_pred == 0) * (y == 0)).sum()\n",
    "\n",
    "    return valid_loss, valid_correct, valid_false_pos, valid_false_neg, valid_true_pos, valid_true_neg\n",
    "\n",
    "def subset_weighted_random_sampler(dataset, idx):\n",
    "    labels = torch.tensor(dataset.y)\n",
    "    subset_labels = labels[idx]\n",
    "    majority_len = int((subset_labels == 0).sum())\n",
    "    minority_len = int((subset_labels == 1).sum())\n",
    "\n",
    "    dist = torch.zeros(len(dataset.y))\n",
    "    dist[idx] = 1\n",
    "    dist[labels == 1] = dist[labels == 1] * (0.5 / minority_len)\n",
    "    dist[labels == 0] = dist[labels == 0] * (0.5 / majority_len)\n",
    "\n",
    "    return WeightedRandomSampler(dist, num_samples=2 * majority_len, replacement=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## K-Fold Cross Validation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def kfold(dataset=dataset, k_folds=5, n_epochs=10, batch_size=64):\n",
    "    history = {'train_loss': [], 'test_loss': [],'train_acc':[],'test_acc':[], 'train_tpr':[], 'test_tpr':[], 'train_fpr':[], 'test_fpr':[]}\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True)\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(np.arange(len(dataset)))):\n",
    "        print('Fold {}'.format(fold + 1))\n",
    "\n",
    "        train_sampler = subset_weighted_random_sampler(dataset, train_idx)\n",
    "        test_sampler = subset_weighted_random_sampler(dataset, val_idx)\n",
    "        train_loader, test_loader = load_data(train_sampler, test_sampler, batch_size=batch_size)\n",
    "\n",
    "        model = LogisticRegression(feature_size=4037)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.002)\n",
    "        #criterion = FocalLoss(gamma=0.2, alpha=0.75)\n",
    "        criterion = nn.BCELoss()\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            train_loss, train_correct, train_fpos, train_fneg, train_tpos, train_tneg = train_epoch(model, train_loader, optimizer, criterion)\n",
    "            test_loss, test_correct, test_fpos, test_fneg, test_tpos, test_tneg = valid_epoch(model, test_loader, criterion)\n",
    "\n",
    "            train_loss = train_loss / len(train_loader.sampler)\n",
    "            train_acc = train_correct / len(train_loader.sampler) * 100\n",
    "            test_loss = test_loss / len(test_loader.sampler)\n",
    "            test_acc = test_correct / len(test_loader.sampler) * 100\n",
    "            train_tpr = train_tpos / (train_tpos + train_fneg)\n",
    "            test_tpr = test_tpos / (test_tpos + test_fneg)\n",
    "            train_fpr = train_fpos / (train_fpos + train_tneg)\n",
    "            test_fpr = test_fpos / (test_fpos + test_tneg)\n",
    "\n",
    "            print(\"Epoch:{}/{} AVG Training Loss:{:.3f} AVG Test Loss:{:.3f} AVG Training Acc {:.2f} % AVG Test Acc {:.2f}%\".format(epoch + 1, n_epochs, train_loss, test_loss, train_acc, test_acc))\n",
    "            print(\"Epoch:{}/{} AVG Training TPR:{:.2f} AVG Test TPR:{:.2f} AVG Training FPR:{:.2f} AVG Test FPR:{:.2f}\".format(epoch + 1, n_epochs, train_tpr, test_tpr, train_fpr, train_fpr))\n",
    "            history['train_loss'].append(train_loss)\n",
    "            history['test_loss'].append(test_loss)\n",
    "            history['train_acc'].append(train_acc)\n",
    "            history['test_acc'].append(test_acc)\n",
    "            history['train_tpr'].append(train_tpr)\n",
    "            history['test_tpr'].append(test_tpr)\n",
    "            history['train_fpr'].append(train_fpr)\n",
    "            history['test_fpr'].append(test_fpr)\n",
    "\n",
    "    return history"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<timed exec>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-4-b230de8ada97>\u001B[0m in \u001B[0;36mkfold\u001B[0;34m(dataset, k_folds, n_epochs, batch_size)\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     16\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mepoch\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mn_epochs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 17\u001B[0;31m             \u001B[0mtrain_loss\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_correct\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_fpos\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_fneg\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_tpos\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_tneg\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtrain_epoch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_loader\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moptimizer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcriterion\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     18\u001B[0m             \u001B[0mtest_loss\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest_correct\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest_fpos\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest_fneg\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest_tpos\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest_tneg\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mvalid_epoch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest_loader\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcriterion\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-3-35bdcb693ce2>\u001B[0m in \u001B[0;36mtrain_epoch\u001B[0;34m(model, train_dataloader, optimizer, criterion)\u001B[0m\n\u001B[1;32m      5\u001B[0m     \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 7\u001B[0;31m     \u001B[0;32mfor\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfeats\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmasks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtrain_dataloader\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      8\u001B[0m         \u001B[0moptimizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mzero_grad\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      9\u001B[0m         \u001B[0my_hat\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfeats\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001B[0m in \u001B[0;36m__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    515\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_sampler_iter\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    516\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_reset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 517\u001B[0;31m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_next_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    518\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_num_yielded\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    519\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_dataset_kind\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0m_DatasetKind\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mIterable\u001B[0m \u001B[0;32mand\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001B[0m in \u001B[0;36m_next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    555\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_next_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    556\u001B[0m         \u001B[0mindex\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_next_index\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# may raise StopIteration\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 557\u001B[0;31m         \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_dataset_fetcher\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfetch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mindex\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# may raise StopIteration\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    558\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_pin_memory\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    559\u001B[0m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_utils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpin_memory\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpin_memory\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001B[0m in \u001B[0;36mfetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     45\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     46\u001B[0m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mpossibly_batched_index\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 47\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcollate_fn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/Projects/CS598/CS598_Project/source/dataloader.py\u001B[0m in \u001B[0;36mcollate_fn\u001B[0;34m(data)\u001B[0m\n\u001B[1;32m     75\u001B[0m             \u001B[0;32mfor\u001B[0m \u001B[0mk_category\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcategory\u001B[0m \u001B[0;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mhour\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     76\u001B[0m                 \u001B[0;32mfor\u001B[0m \u001B[0ml_event\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcategory\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 77\u001B[0;31m                     \u001B[0mx\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi_patient\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mj_hour\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mk_category\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0ml_event\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcategory\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0ml_event\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     78\u001B[0m                     \u001B[0mmasks\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi_patient\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mj_hour\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mk_category\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0ml_event\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     79\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = kfold()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Storing Results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "with open(\"./logistic_model_history.pickle\", \"wb\") as f:\n",
    "    pickle.dump(history, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}